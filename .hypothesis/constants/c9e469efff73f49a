# file: C:\Users\Edwin Boston\AppData\Roaming\Python\Python314\site-packages\transformers\models\auto\modeling_auto.py
# hypothesis_version: 6.150.2

['ASTModel', 'Aimv2Model', 'Aimv2VisionModel', 'AlbertForMaskedLM', 'AlbertForPreTraining', 'AlbertModel', 'AlignModel', 'AltCLIPModel', 'ApertusForCausalLM', 'ApertusModel', 'ArceeForCausalLM', 'ArceeModel', 'AriaModel', 'AriaTextForCausalLM', 'AriaTextModel', 'AutoBackbone', 'AutoModel', 'AutoModelForCTC', 'AutoModelForCausalLM', 'AutoModelForMaskedLM', 'AutoModelWithLMHead', 'AutoformerModel', 'AyaVisionModel', 'BambaForCausalLM', 'BambaModel', 'BarkModel', 'BartForCausalLM', 'BartModel', 'BeitBackbone', 'BeitModel', 'BertForMaskedLM', 'BertForPreTraining', 'BertLMHeadModel', 'BertModel', 'BigBirdForCausalLM', 'BigBirdForMaskedLM', 'BigBirdModel', 'BigBirdPegasusModel', 'BioGptForCausalLM', 'BioGptModel', 'BitBackbone', 'BitModel', 'BitNetForCausalLM', 'BitNetModel', 'BlenderbotModel', 'BlenderbotSmallModel', 'Blip2Model', 'Blip2QFormerModel', 'BlipModel', 'BloomForCausalLM', 'BloomModel', 'BltForCausalLM', 'BltModel', 'BridgeTowerModel', 'BrosModel', 'CLIPModel', 'CLIPSegModel', 'CLIPTextModel', 'CLIPVisionModel', 'CTRLLMHeadModel', 'CTRLModel', 'CamembertForCausalLM', 'CamembertForMaskedLM', 'CamembertModel', 'CanineModel', 'ChameleonModel', 'ChineseCLIPModel', 'ClapModel', 'CodeGenForCausalLM', 'CodeGenModel', 'Cohere2ForCausalLM', 'Cohere2Model', 'Cohere2VisionModel', 'CohereForCausalLM', 'CohereModel', 'ColPaliForRetrieval', 'ColQwen2ForRetrieval', 'ConditionalDetrModel', 'ConvBertForMaskedLM', 'ConvBertModel', 'ConvNextBackbone', 'ConvNextModel', 'ConvNextV2Backbone', 'ConvNextV2Model', 'CpmAntForCausalLM', 'CpmAntModel', 'CvtModel', 'DFineModel', 'DINOv3ConvNextModel', 'DINOv3ViTModel', 'DPRQuestionEncoder', 'DPTModel', 'DabDetrModel', 'DacModel', 'Data2VecAudioForCTC', 'Data2VecAudioModel', 'Data2VecTextModel', 'Data2VecVisionModel', 'DbrxForCausalLM', 'DbrxModel', 'DebertaForMaskedLM', 'DebertaModel', 'DebertaV2ForMaskedLM', 'DebertaV2Model', 'DeepseekV2Model', 'DeepseekV3Model', 'DeepseekVLModel', 'DeformableDetrModel', 'DeiTModel', 'DepthProModel', 'DetaModel', 'DetrForSegmentation', 'DetrModel', 'DiaModel', 'DiffLlamaForCausalLM', 'DiffLlamaModel', 'DinatBackbone', 'DinatModel', 'Dinov2Backbone', 'Dinov2Model', 'DistilBertModel', 'DogeForCausalLM', 'DogeModel', 'DonutSwinModel', 'Dots1ForCausalLM', 'Dots1Model', 'EdgeTamModel', 'EdgeTamVideoModel', 'EdgeTamVisionModel', 'EfficientFormerModel', 'EfficientLoFTRModel', 'EfficientNetModel', 'ElectraForCausalLM', 'ElectraForMaskedLM', 'ElectraModel', 'Emu3ForCausalLM', 'Emu3Model', 'Emu3TextModel', 'EncodecModel', 'EncoderDecoderModel', 'Ernie4_5ForCausalLM', 'Ernie4_5Model', 'Ernie4_5_MoeModel', 'ErnieForCausalLM', 'ErnieForMaskedLM', 'ErnieForPreTraining', 'ErnieMModel', 'ErnieModel', 'EsmForMaskedLM', 'EsmModel', 'EvollaModel', 'Exaone4ForCausalLM', 'Exaone4Model', 'FNetForMaskedLM', 'FNetForPreTraining', 'FNetModel', 'FSMTModel', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconH1Model', 'FalconMambaModel', 'FalconModel', 'FlaubertModel', 'FlavaForPreTraining', 'FlavaModel', 'FlexOlmoForCausalLM', 'FlexOlmoModel', 'Florence2Model', 'FocalNetBackbone', 'FocalNetModel', 'FunnelBaseModel', 'FunnelForMaskedLM', 'FunnelForPreTraining', 'FunnelModel', 'FuyuForCausalLM', 'FuyuModel', 'GLPNModel', 'GPT2LMHeadModel', 'GPT2Model', 'GPTBigCodeModel', 'GPTJForCausalLM', 'GPTJModel', 'GPTNeoForCausalLM', 'GPTNeoModel', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseModel', 'GPTNeoXModel', 'Gemma2ForCausalLM', 'Gemma2Model', 'Gemma3ForCausalLM', 'Gemma3Model', 'Gemma3TextModel', 'Gemma3nAudioEncoder', 'Gemma3nForCausalLM', 'Gemma3nModel', 'Gemma3nTextModel', 'GemmaForCausalLM', 'GemmaModel', 'GitForCausalLM', 'GitModel', 'Glm4ForCausalLM', 'Glm4Model', 'Glm4MoeForCausalLM', 'Glm4MoeModel', 'Glm4vModel', 'Glm4vMoeModel', 'Glm4vMoeTextModel', 'Glm4vTextModel', 'GlmForCausalLM', 'GlmModel', 'GotOcr2Model', 'GptOssForCausalLM', 'GptOssModel', 'GraniteForCausalLM', 'GraniteModel', 'GraniteMoeModel', 'GraphormerModel', 'GroundingDinoModel', 'GroupViTModel', 'HGNetV2Backbone', 'HeliumForCausalLM', 'HeliumModel', 'HieraBackbone', 'HieraForPreTraining', 'HieraModel', 'HubertForCTC', 'HubertModel', 'HunYuanDenseV1Model', 'HunYuanMoEV1Model', 'IBertForMaskedLM', 'IBertModel', 'IJepaModel', 'Idefics2Model', 'Idefics3Model', 'IdeficsModel', 'ImageGPTModel', 'InformerModel', 'InstructBlipModel', 'InternVLModel', 'InternVLVisionModel', 'JambaForCausalLM', 'JambaModel', 'JanusModel', 'JetMoeForCausalLM', 'JetMoeModel', 'JukeboxModel', 'Kosmos2Model', 'Kosmos2_5Model', 'LEDModel', 'LayoutLMForMaskedLM', 'LayoutLMModel', 'LayoutLMv2Model', 'LayoutLMv3Model', 'LevitModel', 'Lfm2ForCausalLM', 'Lfm2Model', 'Lfm2VlModel', 'LiltModel', 'Llama4ForCausalLM', 'Llama4TextModel', 'Llama4VisionModel', 'LlamaForCausalLM', 'LlamaModel', 'LlavaModel', 'LlavaNextModel', 'LlavaNextVideoModel', 'LlavaOnevisionModel', 'LongT5Model', 'LongcatFlashModel', 'LongformerModel', 'LukeForMaskedLM', 'LukeModel', 'LxmertForPreTraining', 'LxmertModel', 'M2M100Model', 'MBartForCausalLM', 'MBartModel', 'MCTCTForCTC', 'MCTCTModel', 'MLCDVisionModel', 'MMGroundingDinoModel', 'MODEL_MAPPING', 'MPNetForMaskedLM', 'MPNetModel', 'MT5EncoderModel', 'MT5Model', 'Mamba2ForCausalLM', 'Mamba2Model', 'MambaForCausalLM', 'MambaModel', 'MarianForCausalLM', 'MarianMTModel', 'MarianModel', 'MarkupLMModel', 'Mask2FormerModel', 'MaskFormerModel', 'MaskFormerSwinModel', 'MegaForCausalLM', 'MegaForMaskedLM', 'MegaModel', 'MegatronBertModel', 'MetaClip2Model', 'MimiModel', 'MiniMaxForCausalLM', 'MiniMaxModel', 'MinistralForCausalLM', 'MinistralModel', 'Mistral3Model', 'MistralForCausalLM', 'MistralModel', 'MixtralForCausalLM', 'MixtralModel', 'MllamaForCausalLM', 'MllamaModel', 'MllamaTextModel', 'MllamaVisionModel', 'MobileBertModel', 'MobileNetV1Model', 'MobileNetV2Model', 'MobileViTModel', 'MobileViTV2Model', 'ModernBertModel', 'MoonshineModel', 'MoshiForCausalLM', 'MoshiModel', 'MptForCausalLM', 'MptModel', 'MraForMaskedLM', 'MraForMultipleChoice', 'MraModel', 'MusicgenForCausalLM', 'MusicgenMelodyModel', 'MusicgenModel', 'MvpForCausalLM', 'MvpModel', 'NatBackbone', 'NatModel', 'NemotronForCausalLM', 'NemotronModel', 'NezhaForMaskedLM', 'NezhaForPreTraining', 'NezhaModel', 'NllbMoeModel', 'NystromformerModel', 'OPTForCausalLM', 'OPTModel', 'Olmo2ForCausalLM', 'Olmo2Model', 'Olmo3ForCausalLM', 'Olmo3Model', 'OlmoForCausalLM', 'OlmoModel', 'OlmoeForCausalLM', 'OlmoeModel', 'OneFormerModel', 'OpenAIGPTLMHeadModel', 'OpenAIGPTModel', 'OpenLlamaForCausalLM', 'OpenLlamaModel', 'Ovis2Model', 'OwlViTModel', 'Owlv2Model', 'PLBartForCausalLM', 'PLBartModel', 'PaliGemmaModel', 'ParakeetEncoder', 'ParakeetForCTC', 'PatchTSMixerModel', 'PatchTSTModel', 'PegasusForCausalLM', 'PegasusModel', 'PegasusXModel', 'PerceiverForMaskedLM', 'PerceiverModel', 'PerceptionEncoder', 'PerceptionLMModel', 'PersimmonForCausalLM', 'PersimmonModel', 'Phi3ForCausalLM', 'Phi3Model', 'Phi4MultimodalModel', 'PhiForCausalLM', 'PhiModel', 'PhimoeForCausalLM', 'PhimoeModel', 'PixtralVisionModel', 'PoolFormerModel', 'ProphetNetModel', 'PvtModel', 'PvtV2Backbone', 'PvtV2Model', 'QDQBertForMaskedLM', 'QDQBertLMHeadModel', 'QDQBertModel', 'Qwen2AudioEncoder', 'Qwen2ForCausalLM', 'Qwen2Model', 'Qwen2MoeForCausalLM', 'Qwen2MoeModel', 'Qwen2VLModel', 'Qwen2VLTextModel', 'Qwen2_5_VLModel', 'Qwen2_5_VLTextModel', 'Qwen3ForCausalLM', 'Qwen3Model', 'Qwen3MoeForCausalLM', 'Qwen3MoeModel', 'Qwen3NextForCausalLM', 'Qwen3NextModel', 'Qwen3VLModel', 'Qwen3VLMoeModel', 'Qwen3VLMoeTextModel', 'Qwen3VLTextModel', 'RTDetrModel', 'RTDetrResNetBackbone', 'RTDetrV2Model', 'RecurrentGemmaModel', 'ReformerForMaskedLM', 'ReformerModel', 'RegNetModel', 'RemBertForCausalLM', 'RemBertForMaskedLM', 'RemBertModel', 'ResNetBackbone', 'ResNetModel', 'RetriBertModel', 'RoCBertForCausalLM', 'RoCBertForMaskedLM', 'RoCBertModel', 'RoFormerForCausalLM', 'RoFormerForMaskedLM', 'RoFormerModel', 'RobertaForCausalLM', 'RobertaForMaskedLM', 'RobertaModel', 'RwkvForCausalLM', 'RwkvModel', 'SEWDForCTC', 'SEWDModel', 'SEWForCTC', 'SEWModel', 'Sam2HieraDetModel', 'Sam2Model', 'Sam2VideoModel', 'Sam2VisionModel', 'SamHQModel', 'SamHQVisionModel', 'SamModel', 'SamVisionModel', 'SeamlessM4TModel', 'SeamlessM4Tv2Model', 'SeedOssForCausalLM', 'SeedOssModel', 'SegGptModel', 'SegformerModel', 'Siglip2Model', 'Siglip2VisionModel', 'SiglipModel', 'SiglipVisionModel', 'SmolLM3ForCausalLM', 'SmolLM3Model', 'SmolVLMModel', 'Speech2TextModel', 'SpeechT5Model', 'SplinterModel', 'SqueezeBertModel', 'StableLmForCausalLM', 'StableLmModel', 'Starcoder2Model', 'SwiftFormerModel', 'Swin2SRModel', 'SwinBackbone', 'SwinModel', 'Swinv2Backbone', 'Swinv2Model', 'T5EncoderModel', 'T5GemmaEncoderModel', 'T5GemmaModel', 'T5Model', 'TapasForMaskedLM', 'TapasModel', 'TextNetBackbone', 'TextNetModel', 'TimesFmModel', 'TimesformerModel', 'TimmBackbone', 'TimmWrapperModel', 'TrOCRForCausalLM', 'TransfoXLLMHeadModel', 'TransfoXLModel', 'TvltForPreTraining', 'TvltModel', 'TvpModel', 'UMT5EncoderModel', 'UMT5Model', 'UdopModel', 'UniSpeechForCTC', 'UniSpeechModel', 'UniSpeechSatForCTC', 'UniSpeechSatModel', 'UnivNetModel', 'VJEPA2Model', 'VanModel', 'VaultGemmaModel', 'ViTHybridModel', 'ViTMAEForPreTraining', 'ViTMAEModel', 'ViTMSNModel', 'ViTModel', 'VideoLlavaModel', 'VideoMAEModel', 'ViltModel', 'VipLlavaModel', 'VisualBertModel', 'VitDetBackbone', 'VitDetModel', 'VitPoseBackbone', 'VitsModel', 'VivitModel', 'VoxtralEncoder', 'Wav2Vec2BertForCTC', 'Wav2Vec2BertModel', 'Wav2Vec2ForCTC', 'Wav2Vec2ForMaskedLM', 'Wav2Vec2ForXVector', 'Wav2Vec2Model', 'WavLMForCTC', 'WavLMForXVector', 'WavLMModel', 'WhisperForCausalLM', 'WhisperModel', 'XCLIPModel', 'XGLMForCausalLM', 'XGLMModel', 'XLMForMultipleChoice', 'XLMModel', 'XLMProphetNetModel', 'XLMRobertaModel', 'XLMRobertaXLModel', 'XLMWithLMHeadModel', 'XLNetLMHeadModel', 'XLNetModel', 'XcodecModel', 'XmodForCausalLM', 'XmodForMaskedLM', 'XmodModel', 'YolosModel', 'YosoForMaskedLM', 'YosoModel', 'Zamba2ForCausalLM', 'Zamba2Model', 'ZambaForCausalLM', 'ZambaModel', 'aimv2', 'aimv2_vision_model', 'albert', 'align', 'altclip', 'apertus', 'arcee', 'aria', 'aria_text', 'audio classification', 'autoformer', 'aya_vision', 'bamba', 'bark', 'bart', 'beit', 'bert', 'bert-generation', 'big_bird', 'bigbird_pegasus', 'biogpt', 'bit', 'bitnet', 'blenderbot', 'blenderbot-small', 'blip', 'blip-2', 'blip_2_qformer', 'bloom', 'blt', 'bridgetower', 'bros', 'camembert', 'canine', 'chameleon', 'chinese_clip', 'clap', 'clip', 'clip_text_model', 'clip_vision_model', 'clipseg', 'clvp', 'code_llama', 'codegen', 'cohere', 'cohere2', 'cohere2_vision', 'colpali', 'colqwen2', 'conditional_detr', 'convbert', 'convnext', 'convnextv2', 'cpmant', 'csm', 'ctrl', 'cvt', 'd_fine', 'dab-detr', 'dac', 'data2vec-audio', 'data2vec-text', 'data2vec-vision', 'dbrx', 'deberta', 'deberta-v2', 'decision_transformer', 'deepseek_v2', 'deepseek_v3', 'deepseek_vl', 'deepseek_vl_hybrid', 'deformable_detr', 'deit', 'depth estimation', 'depth_anything', 'depth_pro', 'deta', 'detr', 'dia', 'diffllama', 'dinat', 'dinov2', 'dinov3_convnext', 'dinov3_vit', 'distilbert', 'doge', 'donut-swin', 'dots1', 'dpr', 'dpt', 'edgetam', 'edgetam_video', 'edgetam_vision_model', 'efficientformer', 'efficientloftr', 'efficientnet', 'electra', 'emu3', 'encodec', 'encoder-decoder', 'eomt', 'ernie', 'ernie4_5', 'ernie4_5_moe', 'ernie_m', 'esm', 'evolla', 'exaone4', 'falcon', 'falcon_h1', 'falcon_mamba', 'flaubert', 'flava', 'flex_olmo', 'florence2', 'fnet', 'focalnet', 'fsmt', 'funnel', 'fuyu', 'gemma', 'gemma2', 'gemma3', 'gemma3_text', 'gemma3n', 'gemma3n_audio', 'gemma3n_text', 'gemma3n_vision', 'git', 'glm', 'glm4', 'glm4_moe', 'glm4v', 'glm4v_moe', 'glm4v_moe_text', 'glm4v_text', 'glpn', 'google-t5/t5-base', 'got_ocr2', 'gpt-sw3', 'gpt2', 'gpt_bigcode', 'gpt_neo', 'gpt_neox', 'gpt_neox_japanese', 'gpt_oss', 'gptj', 'gptsan-japanese', 'granite', 'granite_speech', 'granitemoe', 'granitemoehybrid', 'granitemoeshared', 'graphormer', 'grounding-dino', 'groupvit', 'helium', 'hgnet_v2', 'hiera', 'hubert', 'hunyuan_v1_dense', 'hunyuan_v1_moe', 'ibert', 'idefics', 'idefics2', 'idefics3', 'idefics3_vision', 'ijepa', 'image classification', 'image segmentation', 'imagegpt', 'informer', 'instructblip', 'instructblipvideo', 'internvl', 'internvl_vision', 'jamba', 'janus', 'jetmoe', 'jukebox', 'kosmos-2', 'kosmos-2.5', 'language modeling', 'layoutlm', 'layoutlmv2', 'layoutlmv3', 'led', 'levit', 'lfm2', 'lfm2_vl', 'lightglue', 'lilt', 'llama', 'llama4', 'llama4_text', 'llava', 'llava_next', 'llava_next_video', 'llava_onevision', 'longcat_flash', 'longformer', 'longt5', 'luke', 'lxmert', 'm2m_100', 'mamba', 'mamba2', 'marian', 'markuplm', 'mask2former', 'maskformer', 'maskformer-swin', 'mbart', 'mctct', 'mega', 'megatron-bert', 'metaclip_2', 'mgp-str', 'mimi', 'minimax', 'ministral', 'mistral', 'mistral3', 'mixtral', 'mlcd', 'mllama', 'mm-grounding-dino', 'mobilebert', 'mobilenet_v1', 'mobilenet_v2', 'mobilevit', 'mobilevitv2', 'modernbert', 'modernbert-decoder', 'moonshine', 'moshi', 'mpnet', 'mpt', 'mra', 'mt5', 'multiple choice', 'musicgen', 'musicgen_melody', 'mvp', 'nat', 'nemotron', 'nezha', 'nllb-moe', 'nystromformer', 'object detection', 'olmo', 'olmo2', 'olmo3', 'olmoe', 'omdet-turbo', 'oneformer', 'open-llama', 'openai-gpt', 'opt', 'ovis2', 'owlv2', 'owlvit', 'paligemma', 'parakeet_ctc', 'parakeet_encoder', 'patchtsmixer', 'patchtst', 'pegasus', 'pegasus_x', 'perceiver', 'perception_encoder', 'perception_lm', 'persimmon', 'phi', 'phi3', 'phi4_multimodal', 'phimoe', 'pix2struct', 'pixtral', 'plbart', 'poolformer', 'pop2piano', 'pretraining', 'prophetnet', 'pvt', 'pvt_v2', 'qdqbert', 'question answering', 'qwen2', 'qwen2_5_omni', 'qwen2_5_vl', 'qwen2_5_vl_text', 'qwen2_audio', 'qwen2_audio_encoder', 'qwen2_moe', 'qwen2_vl', 'qwen2_vl_text', 'qwen3', 'qwen3_moe', 'qwen3_next', 'qwen3_omni_moe', 'qwen3_vl', 'qwen3_vl_moe', 'qwen3_vl_moe_text', 'qwen3_vl_text', 'recurrent_gemma', 'reformer', 'regnet', 'rembert', 'resnet', 'retribert', 'roberta', 'roberta-prelayernorm', 'roc_bert', 'roformer', 'rt_detr', 'rt_detr_resnet', 'rt_detr_v2', 'rwkv', 'sam', 'sam2', 'sam2_hiera_det_model', 'sam2_video', 'sam2_vision_model', 'sam_hq', 'sam_hq_vision_model', 'sam_vision_model', 'seamless_m4t', 'seamless_m4t_v2', 'seed_oss', 'segformer', 'seggpt', 'sew', 'sew-d', 'shieldgemma2', 'siglip', 'siglip2', 'siglip2_vision_model', 'siglip_vision_model', 'smollm3', 'smolvlm', 'smolvlm_vision', 'speech_to_text', 'speech_to_text_2', 'speecht5', 'splinter', 'squeezebert', 'stablelm', 'starcoder2', 'superglue', 'superpoint', 'swiftformer', 'swin', 'swin2sr', 'swinv2', 'switch_transformers', 't5', 't5gemma', 'table-transformer', 'tapas', 'textnet', 'timesfm', 'timesformer', 'timm_backbone', 'timm_wrapper', 'token classification', 'transfo-xl', 'trocr', 'tvlt', 'tvp', 'udop', 'umt5', 'unispeech', 'unispeech-sat', 'univnet', 'upernet', 'van', 'vaultgemma', 'video classification', 'video_llava', 'videomae', 'vilt', 'vipllava', 'visual_bert', 'vit', 'vit_hybrid', 'vit_mae', 'vit_msn', 'vitdet', 'vitpose_backbone', 'vits', 'vivit', 'vjepa2', 'voxtral', 'voxtral_encoder', 'wav2vec2', 'wav2vec2-bert', 'wav2vec2-conformer', 'wavlm', 'whisper', 'xLSTMForCausalLM', 'xLSTMModel', 'xclip', 'xcodec', 'xglm', 'xlm', 'xlm-prophetnet', 'xlm-roberta', 'xlm-roberta-xl', 'xlnet', 'xlstm', 'xmod', 'yolos', 'yoso', 'zamba', 'zamba2', 'zoedepth']