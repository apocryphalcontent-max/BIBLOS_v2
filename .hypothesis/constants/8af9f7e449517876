# file: C:\Users\Edwin Boston\AppData\Roaming\Python\Python314\site-packages\transformers\tokenization_utils_fast.py
# hypothesis_version: 6.150.2

[0.0, '-', '.model', 'BPE', 'ByteLevel', 'Sequence', 'Unigram', 'WordLevel', 'WordPiece', '__slow_tokenizer', 'add_prefix_space', 'added_tokens', 'added_tokens.json', 'added_tokens_decoder', 'attention_mask', 'cls', 'config', 'content', 'direction', 'end_of_word_suffix', 'from_slow', 'gguf_file', 'id', 'ids', 'initial_alphabet', 'input_ids', 'length', 'max_length', 'merges', 'model', 'model_type', 'offset_mapping', 'pad_id', 'pad_to_multiple_of', 'pad_token', 'pad_token_type_id', 'pad_type_id', 'padding_side', 'post_processor', 'pre_tokenizer', 'pretokenizers', 'sep', 'special', 'special_tokens', 'special_tokens_mask', 'strategy', 'stride', 'token_type_ids', 'tokenizer', 'tokenizer.json', 'tokenizer.model', 'tokenizer_config', 'tokenizer_file', 'tokenizer_object', 'tokens', 'truncation_side', 'truncation_strategy', 'type', 'unk_id', 'unk_token', 'use_source_tokenizer', 'utf-8', 'vocab', 'vocab_file', 'w']