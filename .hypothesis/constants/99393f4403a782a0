# file: C:\Users\Edwin Boston\AppData\Roaming\Python\Python314\site-packages\transformers\integrations\ggml.py
# hypothesis_version: 6.150.2

['$A', '$B', '</s>', '<0x09>', '<|assistant|>', '<|endoftext|>', '<|end|>', '<|im_end|>', '<|im_start|>', '<|placeholder1|>', '<|placeholder2|>', '<|placeholder3|>', '<|placeholder4|>', '<|placeholder5|>', '<|placeholder6|>', '<|system|>', '<|user|>', 'B', '_model_name_or_path', 'add_prefix_space', 'added_tokens', 'architecture', 'attention.head_count', 'attention.key_length', 'block_count', 'bloom', 'bos_token', 'bos_token_id', 'chat_template', 'context_length', 'conv_L_cache', 'conv_kernel', 'd_ff', 'd_kv', 'd_model', 'deci', 'decilm', 'dummy text', 'embedding_length', 'eos_token', 'eos_token_id', 'expert_count', 'expert_used_count', 'falcon', 'feed_forward_length', 'gemma2', 'gemma3', 'gemma3_text', 'general', 'ggml.bos_token_id', 'ggml.eos_token_id', 'ggml.merges', 'ggml.model', 'ggml.scores', 'ggml.token_type', 'ggml.tokens', 'gpt2', 'head_dim', 'hidden_size', 'intermediate_size', 'layer_norm_eps', 'layer_norm_epsilon', 'legacy', 'lfm2', 'llama', 'mamba', 'merges', 'mistral', 'model_type', 'n_ctx', 'n_embd', 'n_head', 'n_layer', 'n_positions', 'name', 'nemotron', 'norm_eps', 'norm_epsilon', 'num_attention_heads', 'num_experts', 'num_experts_per_tok', 'num_heads', 'num_hidden_layers', 'num_key_value_heads', 'num_layers', 'pad_token', 'pad_token_id', 'phi3', 'qwen2', 'qwen2_moe', 'qwen2moe', 'qwen3', 'qwen3_moe', 'rms_norm_eps', 'rope.dimension_count', 'rope.freq_base', 'rope_theta', 'scores', 'shortconv.l_cache', 'sliding_window', 'ssm.conv_kernel', 'ssm.inner_size', 'ssm.state_size', 'ssm.time_step_rank', 'stablelm', 'starcoder2', 'state_size', 't5', 'time_step_rank', 'token_type', 'tokenizer', 'tokenizer_config', 'tokenizer_type', 'tokens', 'umt5', 'unk_token', 'unk_token_id', 'unknown_token_id', 'vocab_size', '‚ñÅ']