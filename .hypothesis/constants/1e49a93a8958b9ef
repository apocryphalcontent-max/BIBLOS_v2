# file: C:\Users\Edwin Boston\AppData\Roaming\Python\Python314\site-packages\transformers\utils\quantization_config.py
# hypothesis_version: 6.150.2

[1e-06, 0.1, 6.0, 1200.0, 128, 256, 512, 2048, '.', '0.1.7', '0.1.8', '0.11.0', '0.2.0', '0.39.0', '0.4.2', '0.8', '0.8.0', '0.9.0', '1.13.2', '1.15.0', '2.7.9', 'AOBaseConfig', 'QuantizationArgs', 'QuantizationScheme', 'QuantizationStatus', 'abs_max', 'activation_scale_ub', 'aqlm', 'attention', 'auto', 'auto-round', 'auto_gptq', 'auto_round:gptq', 'auto_trainable', 'autoawq', 'autobitlinear', 'autoquant', 'awq', 'axis', 'backend', 'beta1 must be an int', 'beta2 must be an int', 'bf16', 'bfloat16', 'bitlinear', 'bitnet', 'bits must be an int', 'bitsandbytes', 'c4', 'c4-new', 'compressed-tensors', 'config_groups', 'default', 'dense', 'dequantize', 'disable_exllama', 'do_fuse', 'dynamic', 'eetq', 'exllama', 'exllama_config', 'export', 'fbgemm_fp8', 'float16', 'float32', 'float64', 'float8', 'format', 'fp4', 'fp8', 'fp_quant', 'fuse_max_seq_len', 'gemm', 'gemv', 'gptq', 'group_size', 'gsr', 'hadamard', 'hidden_size', 'higgs', 'hqq', 'identity', 'ignore', 'initialized', 'int2', 'int4', 'int4_weight_only', 'int8', 'int8_weight_only', 'ipex', 'kv_cache_scheme', 'layernorm', 'layout', 'llm-awq', 'llm_int8', 'lm_head', 'load_in_4bit', 'load_in_8bit', 'max_batch_size', 'max_input_len', 'max_input_length', 'meta', 'min_kv_scale', 'mlp', 'modules_to_fuse', 'mxfp4', 'nbits', 'nf4', 'num_attention_heads', 'num_key_value_heads', 'nvfp4', 'offline', 'offload_meta', 'online', 'optimum', 'packing_format', 'ptb', 'ptb-new', 'quant_config', 'quant_method', 'quant_scale', 'quant_type', 'quant_type_kwargs', 'quant_zero', 'quantization_config', 'quantization_status', 'quanto', 'quark', 'quest', 'run_compressed', 'skip_modules', 'sparsity_config', 'spqr', 'torch', 'torchao', 'uint8', 'use_alibi', 'use_cuda_fp16', 'use_exllama', 'utf-8', 'version', 'view_as_float', 'vptq', 'w', 'wikitext2', 'zero_point_domain']