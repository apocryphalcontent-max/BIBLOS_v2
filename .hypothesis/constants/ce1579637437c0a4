# file: C:\Users\Edwin Boston\AppData\Roaming\Python\Python314\site-packages\transformers\generation\utils.py
# hypothesis_version: 6.150.2

[-1000000000.0, 0.0, 1.0, 10.0, -100, '/', '0', '4.50.0', 'BarkSemanticModel', 'BaseStreamer', 'HQQ', 'MoshiDepthDecoder', 'PreTrainedModel', '_', '_assisted_decoding', '_beam_search', '_cache', '_eos_token_tensor', '_hf_hook', '_reorder_cache', '_sample', 'assistant_model', 'assistant_tokenizer', 'attention_mask', 'attentions', 'backend', 'cache_implementation', 'cache_params', 'cache_position', 'config', 'cpu', 'cross_attn', 'cuda', 'custom_generate', 'decoder', 'decoder_', 'decoder_input_ids', 'decoder_position_ids', 'disk', 'donut', 'dynamic', 'dynamic_full', 'encoder', 'encoder_ffn_dim', 'encoder_layers', 'encoder_outputs', 'eos_token_id', 'falconh1', 'flash_attention_2', 'generate', 'get_decoder', 'get_seq_length', 'heuristic', 'hf_device_map', 'hf_quantizer', 'hidden_states', 'hybrid', 'input_ids', 'inputs_embeds', 'inputs_tensor', 'is_compileable', 'kwargs', 'labels', 'layers', 'lfm2', 'lfm2-vl', 'logits processor', 'logits_to_keep', 'mamba', 'max_cache_len', 'max_length', 'mems', 'min_length', 'minimax', 'model_kwargs', 'never', 'offloaded', 'offloading', 'output_attentions', 'output_hidden_states', 'past_buckets_states', 'past_key_values', 'position_ids', 'pt', 'quantized', 'quanto', 'reformer', 'return_dict', 'self', 'sliding_window', 'state', 'static', 'stopping criteria', 'streamer', 'synced_gpus', 'to_legacy_cache', 'token_type_ids', 'tokenizer', 'transformers_version', 'trust_remote_code', 'use_cache', 'whisper', 'xlnet']