# =============================================================================
# BIBLOS v2 - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values

# =============================================================================
# GENERAL SETTINGS
# =============================================================================
ENVIRONMENT=development  # development, testing, staging, production
DEBUG=false

# Directory paths
DATA_DIR=./data
OUTPUT_DIR=./output
CACHE_DIR=./cache

# =============================================================================
# DATABASE SETTINGS
# =============================================================================

# Neo4j Graph Database
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password
NEO4J_DATABASE=biblos

# PostgreSQL
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=biblos
POSTGRES_PASSWORD=your_postgres_password
POSTGRES_DATABASE=biblos

# Redis Cache
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0

# Connection Pool
DB_POOL_SIZE=10
DB_MAX_OVERFLOW=20
DB_POOL_TIMEOUT=30

# =============================================================================
# MACHINE LEARNING SETTINGS
# =============================================================================

# Device
ML_DEVICE=cuda  # cuda or cpu
ML_USE_FP16=true

# Model directories
MODELS_DIR=./models
EMBEDDINGS_DIR=./embeddings
CHECKPOINTS_DIR=./checkpoints

# Embedding model
EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
EMBEDDING_DIM=768
MAX_SEQ_LENGTH=512

# Training
ML_BATCH_SIZE=32
ML_LEARNING_RATE=1e-5
ML_NUM_EPOCHS=10
ML_WARMUP_STEPS=1000

# Inference
ML_INFERENCE_BATCH_SIZE=64
ML_MAX_CANDIDATES=100
ML_MIN_CONFIDENCE=0.5

# =============================================================================
# PIPELINE SETTINGS
# =============================================================================

PIPELINE_PARALLEL=true
PIPELINE_MAX_AGENTS=4
PIPELINE_PHASE_TIMEOUT=300

# Quality thresholds
PIPELINE_MIN_CONFIDENCE=0.7
PIPELINE_MIN_COVERAGE=0.9
PIPELINE_GOLD_THRESHOLD=0.9
PIPELINE_SILVER_THRESHOLD=0.75
PIPELINE_BRONZE_THRESHOLD=0.5

# Checkpointing
PIPELINE_CHECKPOINT_INTERVAL=100
PIPELINE_MAX_RETRIES=3

# =============================================================================
# LLM API SETTINGS
# =============================================================================

# OpenAI
OPENAI_API_KEY=your_openai_api_key
OPENAI_MODEL=gpt-4-turbo-preview
OPENAI_TEMPERATURE=0.0
OPENAI_MAX_TOKENS=4096

# Anthropic
ANTHROPIC_API_KEY=your_anthropic_api_key
ANTHROPIC_MODEL=claude-3-opus-20240229

# Local Model (if using)
LOCAL_MODEL_PATH=
USE_LOCAL_MODEL=false

# =============================================================================
# INTEGRATION SETTINGS
# =============================================================================

# Text-Fabric
TF_DATA_DIR=./corpora/tf
TF_CACHE_DIR=./cache/tf

# Macula
MACULA_DATA_DIR=./corpora/macula

# External APIs
BIBLE_API_KEY=
PATRISTIC_API_URL=

# =============================================================================
# LOGGING SETTINGS
# =============================================================================

LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s
LOG_DIR=./logs
LOG_MAX_SIZE=10485760  # 10MB
LOG_BACKUP_COUNT=5
LOG_TO_CONSOLE=true
LOG_TO_FILE=true

# =============================================================================
# API SERVER SETTINGS
# =============================================================================

API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
API_RELOAD=false
CORS_ORIGINS=*
API_KEY=your_api_key
API_RATE_LIMIT=100
